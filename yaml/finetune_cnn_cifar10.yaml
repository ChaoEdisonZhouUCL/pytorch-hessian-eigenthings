# Hyperparameters for ViT on cifar10
# model hyperparams
model_name: "resnet18"
pretrained: True

# # data hyperparams
dataset_name: "cifar10"
data_root: "./data/cifar10"
num_classes: 10

# dataset_name: "cifar100"
# data_root: "./data/cifar100"
# num_classes: 100

# dataset_name: "flowers"
# data_root: "./data/flowers"
# num_classes: 102

# opt hyperparams
optimizer_name: "adamw" #  "nanoadamII", "adamwcollect", "adamw"
save_dir: "./weight_gradient_hist/finetune_resnet"


# train hyperparams
batch_size: 128
num_epochs: 15
learning_rate: !!float 1e-3
weight_decay: 0.0
scheduler_name: None # "cosineannealinglr"
label_smoothing: 0.0 # 0.1
seed: 42
bf16: True
fp16: False

# wandb settings
wandb_project: "Finetune_CIFAR10"
wandb_name: "resnet18_finetune"

# DDP training
ddp: True
